{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14pt\">MIPT, Applied ML, Autumn 2018</span>\n",
    "\n",
    "<span style=\"font-size: 16pt\"> HW #4: Approximate RL homework\n",
    "\n",
    "<span style=\"color:red; font-size: 14pt;\"> Дедлайн 19.11.2018 23:59 </span>\n",
    "\n",
    "<span style=\"color:blue; font-size: 12pt\">Valentin Malykh </span>,\n",
    "<span style=\"color:blue; font-size: 12pt; font-family: 'Verdana'\">val@maly.hk</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Оформление дз**: \n",
    "- Выполненное задание требуется отправлять через <a href='https://goo.gl/forms/XPSIbwp7wPxB4SsI3'>форму </a>\n",
    "\n",
    "- Выполненное дз прикрепляйте в формате файла ``<фамилия>_<группа>_task<номер>.ipynb``, например: ``ivanov_594_task4.ipynb`` \n",
    "\n",
    "**Вопросы**:\n",
    "- Вопросы присылайте в канал в телеграмме ``[Fall 2018]ML Seminars``\n",
    "\n",
    "--------\n",
    "- **PS1**: Будьте внимательны при заполнении формы, когда отправляете ДЗ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Практическое задание (100%)</h1>\n",
    "Описание находится по ссылке: https://gist.github.com/madrugado/1262c3077bf7d8ac8166e4350f0f67e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем сделать как на семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка environment-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,) 4\n"
     ]
    }
   ],
   "source": [
    "print(state_dim, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        # experience == [s, a, r, s', is_end]\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, size=1):\n",
    "        # [s, a, r, s', is_end]\n",
    "        return random.sample(self.buffer, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим DQN-сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow 1.11.0 version required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as tflayers  # Let's make TF simple again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input variables. We'll support multiple states at once\n",
    "current_states = tf.placeholder(dtype=tf.float32,shape=(None,)+state_dim)\n",
    "actions = tf.placeholder(tf.int32,shape=[None])\n",
    "rewards = tf.placeholder(tf.float32,shape=[None])\n",
    "next_states = tf.placeholder(tf.float32, shape=(None,)+state_dim)\n",
    "is_end = tf.placeholder(tf.bool,shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(l_states, scope=None, reuse=False):\n",
    "    assert l_states.get_shape().as_list() == list((None,)+state_dim)\n",
    "    with tf.variable_scope(scope or \"network\") as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        dense_1 = tflayers.fully_connected(l_states, 200, activation_fn=tf.nn.relu)\n",
    "        dense_2 = tflayers.fully_connected(dense_1, 40, activation_fn=tf.nn.relu)\n",
    "        l_qvalues = tflayers.fully_connected(dense_2, n_actions, activation_fn=None)\n",
    "        return l_qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = network(current_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "batch_size = tf.shape(predicted_qvalues)[0]\n",
    "gather_ids = tf.stack([tf.range(batch_size), actions], axis=1)\n",
    "predicted_qvalues_for_actions = tf.gather_nd(predicted_qvalues, gather_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_qvalues_for_actions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_next_qvalues = network(next_states, reuse=True)\n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = tf.add(rewards, tf.multiply(gamma, tf.reduce_max(predicted_next_qvalues, axis=1)))\n",
    "target_qvalues_for_actions = tf.where(\n",
    "    is_end, \n",
    "    tf.zeros_like(target_qvalues_for_actions),\n",
    "    target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = tf.losses.mean_squared_error(target_qvalues_for_actions, predicted_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network updates. Note the small learning rate (for stability)\n",
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(\n",
    "    loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"network\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow feature - session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow feature 2 - variables initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check all your valiables by:\n",
    "# [v.name for v in tf.trainable_variables()]\n",
    "# they should all starts with \"network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(exp_buffer, t_max=1000, train_freq=8, batch_size=16, print_result=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # get action q-values from the network\n",
    "        q_values = sess.run(\n",
    "            predicted_qvalues, \n",
    "            feed_dict={current_states: np.array([s])}\n",
    "        )[0]\n",
    "        \n",
    "        a = np.random.choice(\n",
    "            [np.argmax(q_values), np.random.randint(n_actions)], p=[1 - epsilon, epsilon]\n",
    "        )\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        exp_buffer.add([s, a, r, new_s, done])\n",
    "        \n",
    "        if t % train_freq == 0 and len(exp_buffer.buffer) >= batch_size:\n",
    "            # train agent one step. Note that we use one-element arrays instead of scalars \n",
    "            # because that's what function accepts.\n",
    "            exps = exp_buffer.sample(size=batch_size)\n",
    "            s_s = np.array([exp[0] for exp in exps])\n",
    "            a_s = np.array([exp[1] for exp in exps])\n",
    "            r_s = np.array([exp[2] for exp in exps])\n",
    "            ns_s = np.array([exp[3] for exp in exps])\n",
    "            ends = np.array([exp[4] for exp in exps])\n",
    "            curr_loss, _ = sess.run(\n",
    "                [loss, train_step], \n",
    "                feed_dict={\n",
    "                    current_states: s_s,\n",
    "                    actions: a_s,\n",
    "                    rewards: r_s,\n",
    "                    next_states: ns_s,\n",
    "                    is_end: ends,\n",
    "                })\n",
    "            total_loss += curr_loss\n",
    "\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: \n",
    "            break\n",
    "    if print_result:        \n",
    "        if r >= 0:\n",
    "            print(\"Landed successfully\")\n",
    "        if r < 0:\n",
    "            print(\"Failed\")\n",
    "            \n",
    "    return total_reward, total_loss/float(t), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inial_epsilon = epsilon = 0.2\n",
    "final_epsilon = 0.01\n",
    "n_epochs = 20\n",
    "exp_buffer = ExperienceBuffer(buffer_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00529394,  1.4131014 ,  0.536199  ,  0.09693115, -0.00612753,\n",
       "       -0.1214569 ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune model from epsilon=0.1\n",
    "epsilon = 0.1\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d8d201111f472f87ed5712957271c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = -477.392\tepsilon = 0.095\tloss = 7.601\tsteps = 68.403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ecd5e458568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                  print_result=False)\n\u001b[0;32m----> 9\u001b[0;31m                 for _ in range(300)] #generate new sessions\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msession_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1ecd5e458568>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                  print_result=False)\n\u001b[0;32m----> 9\u001b[0;31m                 for _ in range(300)] #generate new sessions\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msession_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a2d58611b890>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(exp_buffer, t_max, train_freq, batch_size, print_result)\u001b[0m\n\u001b[1;32m     10\u001b[0m         q_values = sess.run(\n\u001b[1;32m     11\u001b[0m             \u001b[0mpredicted_qvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         )[0]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0mfeed_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_dict_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten_dict_items\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0mflat_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_dictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mIsSequence\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \"\"\"\n\u001b[0;32m-> 2084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mIsNamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "n_pos_epochs = 0\n",
    "for i in tqdm_notebook(range(n_epochs)):\n",
    "    sessions = [generate_session(exp_buffer,\n",
    "                                 t_max=1000,\n",
    "                                 train_freq=8,\n",
    "                                 batch_size=16,\n",
    "                                 print_result=False)\n",
    "                for _ in range(300)] #generate new sessions\n",
    "    session_rewards, session_loss, session_steps = map(np.array, zip(*sessions))\n",
    "    \n",
    "    #epsilon -= (inial_epsilon - final_epsilon) / float(n_epochs)\n",
    "    if epsilon > final_epsilon:\n",
    "        epsilon = epsilon * 0.95\n",
    "    \n",
    "    print(\"mean reward = {:.3f}\\tepsilon = {:.3f}\\tloss = {:.3f}\\tsteps = {:.3f}\".format(\n",
    "        np.mean(session_rewards), epsilon, np.mean(session_loss), np.mean(session_steps)))\n",
    "    \n",
    "    # If we won three times in a row, stop learning\n",
    "    if np.mean(session_rewards) > 0:\n",
    "        n_pos_epochs += 1\n",
    "    else:\n",
    "        n_pos_epochs = 0\n",
    "    if n_pos_epochs >= 3:\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Протестим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "#env.reset()\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session(exp_buffer, t_max=1000, train_freq=10000, batch_size=1, print_result=True) for _ in range(10)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Садится успешно 5 раз из 10. Неплохо для такой тупой архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
