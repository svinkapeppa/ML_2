{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rubanenko_595_task5.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"np7RdB_oVCbj","colab_type":"text"},"cell_type":"markdown","source":["# Homework №5\n","\n","Рубаненко Евгений, 595"]},{"metadata":{"id":"X3KtM1imajMj","colab_type":"text"},"cell_type":"markdown","source":["## Подготовка"]},{"metadata":{"id":"7dPBHfwPRdVl","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt install ffmpeg xvfb python-opengl -y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HCuU-fF_Pn5q","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip3 install gym\n","!pip3 install pyvirtualdisplay\n","!pip3 install piglet\n","!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","!pip3 install torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gm3c4TDyRgqn","colab_type":"code","colab":{}},"cell_type":"code","source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"36F502ekRh4E","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iG0R8F_-VNrU","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import random\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from collections import deque\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor, LongTensor\n","    device = torch.device('cuda')\n","else:\n","    from torch import FloatTensor, LongTensor\n","    device = torch.device('cpu')\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eykcYcg-RBxZ","colab_type":"code","outputId":"a2b9012e-0b08-438a-b739-8579cfa02ad8","executionInfo":{"status":"ok","timestamp":1543946156002,"user_tz":-180,"elapsed":119242,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"cell_type":"code","source":["env = gym.make(\"BipedalWalker-v2\").unwrapped\n","\n","plt.imshow(env.render('rgb_array'))\n","print(\"Observation space:\", env.observation_space)\n","print(\"Action space:\", env.action_space)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","Observation space: Box(24,)\n","Action space: Box(4,)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f572f11f8d0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"qavOI-rExVvB","colab_type":"text"},"cell_type":"markdown","source":["## Решение"]},{"metadata":{"id":"53pUciMFzTFY","colab_type":"text"},"cell_type":"markdown","source":["Данное задание предлагалось решать с помощью `Actor-Critic`. Чтобы разобраться в теме я прочитал следующие статьи:\n","1. https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752  \n","    Данная статья дала мне общее понимание того, в чем заключается идея `Actor-Critic`.\n","2. https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d  \n","    В этой статье я прочитал про математическую составляющую `Actor-Critic`.\n","3. https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69  \n","    Объяснение алгоритма с примером кода.\n","4. https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py  \n","    Пример из официального репозитория `pytorch`.\n","\n","Дальше я уже стал читать про `BipedalWalker-v2`. Где-то я прочитал, что лучшие результаты сейчас получены с помощью `DQN` и `DDPG`.\n","Оказалось, что `DDPG` -- это усложненный вариант `Actor-Critic`, поэтому я стал читать про этот алгоритм:\n","1. https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html  \n","    Объяснение алгоритма с примером кода.\n","2. https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html  \n","    Объяснение алгоритма с примером кода.\n","2. https://arxiv.org/pdf/1509.02971.pdf  \n","    Оригинальная статья, в которой был представлен `DDPG`."]},{"metadata":{"id":"7C-k00DAt3JI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Гиперпараметры\n","MAX_EPISODES = 5000         #  Максимальное число эпизодов\n","MAX_TIMESTEPS = 2000        #  Максимальное число шагов в эпизоде\n","BATCH_SIZE = 100            #  Сколько элементов сэмплируется из памяти\n","GAMMA = 0.99                #  Используется при обновлении reward\n","POLICY_NOISE = 0.2          #  Отвеачет за шум при обновлении политики\n","NOISE_CLIP = 0.5            #  Границы шума\n","POLICY_DELAY = 2            #  Обновление политики происходит не на каждом шаге\n","TAU = 0.005                 #  Параметр обновления весов\n","EXPLORATION_NOISE = 0.1     #  Отвечает за шум при выборе действия"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9525J8Zso9M4","colab_type":"code","colab":{}},"cell_type":"code","source":["# В DDPG обновление весов происходит по следующей формуле\n","# \\theta^{\\mu'} = \\theta^{\\mu'} * (1 - \\tau) + \\theta^{\\mu} * \\tau\n","def _update(target, source, tau):\n","    for target_param, source_param in zip(target.parameters(), source.parameters()):\n","        target_param.data.copy_(target_param.data * (1. - tau) + source_param.data * tau)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6cRQvlyEo9PN","colab_type":"code","colab":{}},"cell_type":"code","source":["# Простая реализация ReplayBuffer.\n","# Практика показала, что можно не ограничивать размер буфера.\n","class Memory:\n","    def __init__(self):\n","        self.buffer = []\n","\n","    def add(self, transition):\n","        self.buffer.append(transition)\n","\n","    def sample(self, batch_size):\n","        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n","        state, action, reward, next_state, done = [], [], [], [], []\n","\n","        for i in indexes:\n","            s, a, r, s_, d = self.buffer[i]\n","            state.append(np.array(s, copy=False))\n","            action.append(np.array(a, copy=False))\n","            reward.append(np.array(r, copy=False))\n","            next_state.append(np.array(s_, copy=False))\n","            done.append(np.array(d, copy=False))\n","\n","        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4EMfWvpmpm4a","colab_type":"code","colab":{}},"cell_type":"code","source":["class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim, max_action):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim, 400)\n","        self.fc2 = nn.Linear(400, 300)\n","        self.fc3 = nn.Linear(300, action_dim)\n","\n","        self.max_action = max_action\n","\n","    def forward(self, state):\n","        action = F.relu(self.fc1(state))\n","        action = F.relu(self.fc2(action))\n","        action = torch.tanh(self.fc3(action)) * self.max_action\n","        \n","        return action"],"execution_count":0,"outputs":[]},{"metadata":{"id":"56nLUZQBp-6q","colab_type":"code","colab":{}},"cell_type":"code","source":["class Critic(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n","        self.fc2 = nn.Linear(400, 300)\n","        self.fc3 = nn.Linear(300, 1)\n","\n","    def forward(self, state, action):\n","        pair = torch.cat([state, action], dim=1)\n","\n","        qvalue = F.relu(self.fc1(pair))\n","        qvalue = F.relu(self.fc2(qvalue))\n","        qvalue = self.fc3(qvalue)\n","\n","        return qvalue"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ytNa204AqSSs","colab_type":"code","colab":{}},"cell_type":"code","source":["# Основная модель.\n","# Изначально в DDPG предлагалось использовать две пары Actor-Critic.\n","# Здесь реализовано в виде Actor(1)-Critic(2) (Идея из A3C).\n","# Оптимизатор -- Adam -- использовался в оригинальной статье\n","# (мне самому Adam нравится больше остальных, потому что он довольно\n","# агрессивный в начале).\n","class DDPG:\n","    def __init__(self, state_dim, action_dim, max_action, batch_size, gamma,\n","                 tau, policy_noise, noise_clip, policy_delay):\n","        self.batch_size = batch_size\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.policy_noise = policy_noise\n","        self.noise_clip = noise_clip\n","        self.policy_delay = policy_delay\n","        \n","        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","        self.actor_target.load_state_dict(self.actor.state_dict())\n","        self.actor_optimizer = optim.Adam(self.actor.parameters())\n","\n","        self.critic_1 = Critic(state_dim, action_dim).to(device)\n","        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n","        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n","        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters())\n","\n","        self.critic_2 = Critic(state_dim, action_dim).to(device)\n","        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n","        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n","        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters())\n","\n","        self.max_action = max_action\n","\n","    def select_action(self, state):\n","        state = FloatTensor(state.reshape(1, -1))\n","        return self.actor(state).cpu().data.numpy().flatten()\n","\n","    def update(self, memory, n_iter):\n","        for i in range(n_iter):\n","            state, action_, reward, next_state, done = memory.sample(self.batch_size)\n","            state = FloatTensor(state)\n","            action = FloatTensor(action_)\n","            reward = FloatTensor(reward).reshape((self.batch_size, 1))\n","            next_state = FloatTensor(next_state)\n","            done = FloatTensor(done).reshape((self.batch_size, 1))\n","\n","            noise = FloatTensor(action_).data.normal_(0, self.policy_noise)\n","            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n","            next_action = self.actor_target(next_state) + noise\n","            next_action = next_action.clamp(-self.max_action, self.max_action)\n","\n","            target_Q1 = self.critic_1_target(next_state, next_action)\n","            target_Q2 = self.critic_2_target(next_state, next_action)\n","            target_Q = torch.min(target_Q1, target_Q2)\n","            target_Q = reward + ((1 - done) * self.gamma * target_Q).detach()\n","\n","            current_Q1 = self.critic_1(state, action)\n","            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n","            self.critic_1_optimizer.zero_grad()\n","            loss_Q1.backward()\n","            self.critic_1_optimizer.step()\n","\n","            current_Q2 = self.critic_2(state, action)\n","            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n","            self.critic_2_optimizer.zero_grad()\n","            loss_Q2.backward()\n","            self.critic_2_optimizer.step()\n","\n","            if i % self.policy_delay == 0:\n","                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n","\n","                self.actor_optimizer.zero_grad()\n","                actor_loss.backward()\n","                self.actor_optimizer.step()\n","\n","                _update(self.actor_target, self.actor, self.tau)\n","                _update(self.critic_1_target, self.critic_1, self.tau)\n","                _update(self.critic_2_target, self.critic_2, self.tau)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5oGuTYt7qq41","colab_type":"code","colab":{}},"cell_type":"code","source":["# Обучение модели (стандартное итерирование по эпизодам / шагам)\n","def train():\n","    env = gym.make('BipedalWalker-v2')\n","    state_dim = env.observation_space.shape[0]\n","    action_dim = env.action_space.shape[0]\n","    max_action = float(env.action_space.high[0])\n","\n","    ddpg = DDPG(\n","        state_dim=state_dim,\n","        action_dim=action_dim,\n","        max_action=max_action,\n","        batch_size=BATCH_SIZE,\n","        gamma=GAMMA,\n","        tau=TAU,\n","        policy_noise=POLICY_NOISE,\n","        noise_clip=NOISE_CLIP,\n","        policy_delay=POLICY_DELAY\n","    )\n","    memory = Memory()\n","\n","    avg_reward = 0\n","\n","    for episode in range(1, MAX_EPISODES + 1):\n","        state = env.reset()\n","\n","        for t in range(MAX_TIMESTEPS):\n","            action = ddpg.select_action(state)\n","            action = action + np.random.normal(0, EXPLORATION_NOISE, size=action_dim)\n","            action = action.clip(env.action_space.low, env.action_space.high)\n","\n","            next_state, reward, done, _ = env.step(action)\n","            memory.add((state, action, reward, next_state, float(done)))\n","            state = next_state\n","\n","            avg_reward += reward\n","\n","            if done or t == (MAX_TIMESTEPS - 1):\n","                ddpg.update(memory, t)\n","                break\n","\n","        if (avg_reward / 10) >= 300:\n","            print('Solved!')\n","            break\n","\n","        if episode % 10 == 0:\n","            print('Episode: {}\\tAverage Reward: {:.2f}'.format(episode, avg_reward / 10))\n","            avg_reward = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L7uOlXsqxaqv","colab_type":"code","outputId":"52ffea52-19b0-4622-e28c-4fb99a9f6666","executionInfo":{"status":"ok","timestamp":1543959789004,"user_tz":-180,"elapsed":6359603,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":1805}},"cell_type":"code","source":["# Обучение занимает где-то 4-5 часов\n","# Оно не очень стабильно (иногда из-за плохого начала сходится очень медленно)\n","train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","Episode: 10\tAverage Reward: -116.64\n","Episode: 20\tAverage Reward: -129.62\n","Episode: 30\tAverage Reward: -113.84\n","Episode: 40\tAverage Reward: -117.32\n","Episode: 50\tAverage Reward: -118.74\n","Episode: 60\tAverage Reward: -140.47\n","Episode: 70\tAverage Reward: -131.91\n","Episode: 80\tAverage Reward: -139.98\n","Episode: 90\tAverage Reward: -115.80\n","Episode: 100\tAverage Reward: -112.53\n","Episode: 110\tAverage Reward: -109.13\n","Episode: 120\tAverage Reward: -119.49\n","Episode: 130\tAverage Reward: -118.80\n","Episode: 140\tAverage Reward: -117.31\n","Episode: 150\tAverage Reward: -136.19\n","Episode: 160\tAverage Reward: -136.62\n","Episode: 170\tAverage Reward: -110.35\n","Episode: 180\tAverage Reward: -101.55\n","Episode: 190\tAverage Reward: -116.47\n","Episode: 200\tAverage Reward: -114.29\n","Episode: 210\tAverage Reward: -105.17\n","Episode: 220\tAverage Reward: -83.95\n","Episode: 230\tAverage Reward: -78.24\n","Episode: 240\tAverage Reward: -60.19\n","Episode: 250\tAverage Reward: -65.66\n","Episode: 260\tAverage Reward: -8.99\n","Episode: 270\tAverage Reward: -84.67\n","Episode: 280\tAverage Reward: 48.07\n","Episode: 290\tAverage Reward: 49.67\n","Episode: 300\tAverage Reward: -33.02\n","Episode: 310\tAverage Reward: 36.09\n","Episode: 320\tAverage Reward: 134.49\n","Episode: 330\tAverage Reward: 173.12\n","Episode: 340\tAverage Reward: 162.74\n","Episode: 350\tAverage Reward: 164.89\n","Episode: 360\tAverage Reward: 168.00\n","Episode: 370\tAverage Reward: 113.72\n","Episode: 380\tAverage Reward: 219.35\n","Episode: 390\tAverage Reward: 41.28\n","Episode: 400\tAverage Reward: 50.24\n","Episode: 410\tAverage Reward: 264.92\n","Episode: 420\tAverage Reward: 274.44\n","Episode: 430\tAverage Reward: 230.16\n","Episode: 440\tAverage Reward: 250.65\n","Episode: 450\tAverage Reward: 249.73\n","Episode: 460\tAverage Reward: 221.67\n","Episode: 470\tAverage Reward: 282.90\n","Episode: 480\tAverage Reward: 238.94\n","Episode: 490\tAverage Reward: 239.15\n","Episode: 500\tAverage Reward: 288.17\n","Episode: 510\tAverage Reward: 287.07\n","Episode: 520\tAverage Reward: 256.14\n","Episode: 530\tAverage Reward: 231.20\n","Episode: 540\tAverage Reward: 269.79\n","Episode: 550\tAverage Reward: 265.61\n","Episode: 560\tAverage Reward: 285.42\n","Episode: 570\tAverage Reward: 236.35\n","Episode: 580\tAverage Reward: 245.09\n","Episode: 590\tAverage Reward: 239.82\n","Episode: 600\tAverage Reward: 268.96\n","Episode: 610\tAverage Reward: 262.26\n","Episode: 620\tAverage Reward: 291.27\n","Episode: 630\tAverage Reward: 290.46\n","Episode: 640\tAverage Reward: 286.61\n","Episode: 650\tAverage Reward: 267.27\n","Episode: 660\tAverage Reward: 266.63\n","Episode: 670\tAverage Reward: 288.80\n","Episode: 680\tAverage Reward: 293.38\n","Episode: 690\tAverage Reward: 296.65\n","Episode: 700\tAverage Reward: 292.47\n","Episode: 710\tAverage Reward: 292.06\n","Episode: 720\tAverage Reward: 264.72\n","Episode: 730\tAverage Reward: 278.66\n","Episode: 740\tAverage Reward: 250.58\n","Episode: 750\tAverage Reward: 265.47\n","Episode: 760\tAverage Reward: 266.33\n","Episode: 770\tAverage Reward: 294.24\n","Episode: 780\tAverage Reward: 179.13\n","Episode: 790\tAverage Reward: 294.42\n","Episode: 800\tAverage Reward: 294.19\n","Episode: 810\tAverage Reward: 293.53\n","Episode: 820\tAverage Reward: 293.84\n","Episode: 830\tAverage Reward: 262.57\n","Episode: 840\tAverage Reward: 291.46\n","Episode: 850\tAverage Reward: 294.70\n","Episode: 860\tAverage Reward: 294.96\n","Episode: 870\tAverage Reward: 274.58\n","Episode: 880\tAverage Reward: 257.64\n","Episode: 890\tAverage Reward: 261.45\n","Episode: 900\tAverage Reward: 249.26\n","Episode: 910\tAverage Reward: 241.92\n","Episode: 920\tAverage Reward: 275.53\n","Episode: 930\tAverage Reward: 298.95\n","Episode: 940\tAverage Reward: 271.26\n","Episode: 950\tAverage Reward: 229.56\n","Episode: 960\tAverage Reward: 297.23\n","Episode: 970\tAverage Reward: 269.55\n","Episode: 980\tAverage Reward: 297.07\n","Episode: 990\tAverage Reward: 296.30\n","Solved!\n"],"name":"stdout"}]},{"metadata":{"id":"i257OqIcW0qk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}